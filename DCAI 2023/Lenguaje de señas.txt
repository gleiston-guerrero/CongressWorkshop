Aunque investigadores han tratado de apalear los problemas que PHaSIs afrontan al tratar de comunicarse con el resto de la sociedad y vice versa, la comunicación entre PHaSIs se ha dado desde hace muchos años atrás utilizando un lenguaje propio de este grupo de personas como lo es el lenguaje de señas con las manos (lenguaje de señas). Sin embargo, este lenguaje debe ser aprendido por quién desee utilizarlo, y varia según el país. Es decir, cada país tiene su versión del lengauje de señas que es utilizado por su población.

En cuanto a la traducción del lenguaje español de señas se ha recuperado el documento que presenta el trabajo de  Galván-Ruiz et al. [Robust Identification System for Spanish Sign Language Based on Three-Dimensional Frame Information] en el que han usado un sensor volumétrico Leap Motion por su capacidad de reconocer los movimientos de la mano en 3 dimensiones. El sistema ha sidfo alimentado con la grabación de 176 palabras dinámicas tomadas directamente de un sujeto con discapacidad auditiva.

Para poder aprender el lenguaje de señas se han desarrollador algunos traductores, entre ellos se puede citar al desarrollado por Ahmed et al.[Arabic sign language intelligent translator] con cuyo trabajo pretenden facilitar a la comunidad de los países árabes comprender el lenguaje de señas mediante la traducción automática de gestos dinámicos aislados, dividos en cinco clases: alfabeto, números, palabras significativas, expresiones árabes, y sustantivos y verbos.


Otro trabajo en esta misma línea que se puede citar es el trabajo de Yalçin et al. [Turkish sign language alphabet translator] que consiste en un guante capaz de detectar el movimiento de la mano y el movimiento de los dedos. Con este sistema Yalçin et al. [Turkish sign language alphabet translator] traduce del lenguaje de señas a texto, y obtuvieron con éxito el reconocimiento de 18 letras del alfabeto turko. Por otro lado, el guante diseñado por Yeasin et al. [Design and Implementation of Bangla Sign Language Translator] traduce el lenguaje de signos al habla Bangla mediante el reconocimiento de las señas. El trabajo de Yeasin et al.[Design and Implementation of Bangla Sign Language Translator] está orientado para la comunicación entre las personas sordas y personas que pueden oir pero tienen alguna discapacidad del habla. El habla emitido por el guante le puede sirvir como confirmación del mensaje para el usuario hablante, en vista de qué para poder emitir el mensaje para la persona sorda debe hacerlo por medio del lenguaje de señas.

El lenguaje que mayor importancia ha tomado entre los investigadores es el lenguaje americano de señas. Entre los traductores de lenguaje americano de señas que se han desarrollado están los trabajos de Joshi et al. [American sign language translation using edge detection and cross correlation] que traduce señas del lenguaje de señas americano corrspondientes a caracteres del alfabeto, palabras y frases completas al lenguaje textual en inglés. Otro trabajo es el presentado por Jin et al. [A mobile application of American sign language translation via image processing algorithms] proponen un marco basado en técnicas de procesamiento de imágenes para reconocer varios gestos basándose en imágenes de las señales de las manos. El marco propuesto por Jin et al. [A mobile application of American sign language translation via image processing algorithms] lo han implementado en smartphones, el mismo que ha sido capaz de reconocer 16 gestos del lenguaje americano de señas.

Esta misma línea sigue el trabajo de Guo et al. [Sign language recognition based on adaptive HMMS with data augmentation] en el que presentan un sistema de reconocimiento del alfabeto del lenguaje americano de señasque utiliza imágenes de profundidad capturadas con el sensor de profundidad de Microsoft Kinect. El trabajo de Guo et al. [Sign language recognition based on adaptive HMMS with data augmentation] se diferencia de los anteriores por utilizar y analizar imágenes  de profundidad. Así mismo, Pigou et al. [Sign Language Recognition Using Convolutional Neural Networks] han contribuido con unsistema de reconocimiento que utiliza Microsoft Kinect y es capaz de reconocer 20 gestos italianos. Para la traducción del lenguaje de señas de la india han trabajo Rao \& Kishore [Sign Language Recognition System Simulated for Video Captured with Smart Phone Front Camera]. Con el sistema propuesto por Rao \& Kishore [Sign Language Recognition System Simulated for Video Captured with Smart Phone Front Camera], las personas pueden comunicarse con la ayuda  de sus smartphones grabando su vídeo de signos con su cámara frontal, y enviándolo a la persona receptora del mensaje. El sistema descodificará ese vídeo enviado traduciendo los signos en texto.

Morocho Cayamcela \& Lim [Fine-tuning a pre-trained Convolutional Neural Network Model to translate American Sign Language in Real-time] pretenden traducir los signos correspondientes al alfabeto americano.


